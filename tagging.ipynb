{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import childespy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "FULL_SAMPLED_TOKENS_CSV_NAME = 'sampled_full.csv'\n",
    "TAGGED_SAMPLED_TOKENS_CSV_NAME = 'sampled_tagged.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport stanza\\n\\ncorenlp_dir = \\'./corenlp\\'\\nstanza.install_corenlp(dir=corenlp_dir)\\n\\n# Set the CORENLP_HOME environment variable to point to the installation location\\nimport os\\nos.environ[\"CORENLP_HOME\"] = corenlp_dir\\n\\nfrom stanza.server import CoreNLPClient\\n\\n# https://stanfordnlp.github.io/stanza/client_properties.html\\n\\nclient = CoreNLPClient(\\n    annotators=[\\'tokenize\\',\\'ssplit\\', \\'pos\\', \\'depparse\\'], \\n    memory=\\'4G\\', \\n    endpoint=\\'http://localhost:9001\\',\\n    be_quiet=True)\\nprint(client)\\n\\ndef get_nlp_tokenizer(utt):\\n    # nlp(utt.replace(\"_\", \" \").replace(\"+\", \" \"))._sentences[0]\\n    info = client.annotate(utt.replace(\"_\", \" \").replace(\"+\", \" \"))\\n    return [{\\'text\\':token.word, \\'pos\\':token.pos, \\'morph\\':\\'\\'} for token in info.sentence[0].token]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### set up get_nlp_tokenizer such that it returns a tagger's parse of an utterance without '_' or '+' ###\n",
    "\"\"\"\n",
    "the return value of get_nlp_tokenizer should be in the form\n",
    "[{text: text, pos_: pos, morph: morph} for token in utterance]\n",
    "\"\"\"\n",
    "\n",
    "# spacy\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_nlp_tokenizer(utterance):\n",
    "    info = nlp(utterance.replace(\"_\", \" \").replace(\"+\", \" \"))\n",
    "    return [{'text':word.text, 'pos':word.pos_,'morph':'|'.join(word.morph)} for word in info]\n",
    "\n",
    "\n",
    "# stanza\n",
    "''' \n",
    "import stanza\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def get_nlp_tokenizer(utt):\n",
    "    info = nlp(utt.replace(\"_\", \" \").replace(\"+\", \" \"))\n",
    "    all_tokens = []\n",
    "    for sent in info.sentences:\n",
    "        all_tokens.extend([{'text':word.text, 'pos':word.upos, 'morph':word.feats} for word in sent.words])\n",
    "    return all_tokens\n",
    "'''\n",
    "\n",
    "# corenlp\n",
    "'''\n",
    "import stanza\n",
    "\n",
    "corenlp_dir = './corenlp'\n",
    "stanza.install_corenlp(dir=corenlp_dir)\n",
    "\n",
    "# Set the CORENLP_HOME environment variable to point to the installation location\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = corenlp_dir\n",
    "\n",
    "from stanza.server import CoreNLPClient\n",
    "\n",
    "# https://stanfordnlp.github.io/stanza/client_properties.html\n",
    "\n",
    "client = CoreNLPClient(\n",
    "    annotators=['tokenize','ssplit', 'pos', 'depparse'], \n",
    "    memory='4G', \n",
    "    endpoint='http://localhost:9001',\n",
    "    be_quiet=True)\n",
    "print(client)\n",
    "\n",
    "def get_nlp_tokenizer(utt):\n",
    "    # nlp(utt.replace(\"_\", \" \").replace(\"+\", \" \"))._sentences[0]\n",
    "    info = client.annotate(utt.replace(\"_\", \" \").replace(\"+\", \" \"))\n",
    "    return [{'text':token.word, 'pos':token.pos, 'morph':''} for token in info.sentence[0].token]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PROCESSING SAMPLED_FULL_FRENCH BY SPLITTING TOKENS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>language</th>\n",
       "      <th>token_order</th>\n",
       "      <th>prefix</th>\n",
       "      <th>stem</th>\n",
       "      <th>actual_phonology</th>\n",
       "      <th>model_phonology</th>\n",
       "      <th>suffix</th>\n",
       "      <th>num_morphemes</th>\n",
       "      <th>...</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>target_child_id</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>log_frequency</th>\n",
       "      <th>log_frequency_bin</th>\n",
       "      <th>utterance_gloss</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62438075</td>\n",
       "      <td>this</td>\n",
       "      <td>eng</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22744</td>\n",
       "      <td>22743</td>\n",
       "      <td>42448</td>\n",
       "      <td>17084600</td>\n",
       "      <td>10570</td>\n",
       "      <td>9.265775</td>\n",
       "      <td>4</td>\n",
       "      <td>this one looks like she's driving a</td>\n",
       "      <td>pro:dem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61402650</td>\n",
       "      <td>need</td>\n",
       "      <td>eng</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>need</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22707</td>\n",
       "      <td>22704</td>\n",
       "      <td>42252</td>\n",
       "      <td>16839687</td>\n",
       "      <td>2591</td>\n",
       "      <td>7.859799</td>\n",
       "      <td>4</td>\n",
       "      <td>now I need</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62051390</td>\n",
       "      <td>anything</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>anything</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22729</td>\n",
       "      <td>22728</td>\n",
       "      <td>42378</td>\n",
       "      <td>16986849</td>\n",
       "      <td>407</td>\n",
       "      <td>6.008813</td>\n",
       "      <td>2</td>\n",
       "      <td>did we see anything else when we were there</td>\n",
       "      <td>pro:indef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61453077</td>\n",
       "      <td>merrily</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>merry</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dadj LY</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42274</td>\n",
       "      <td>16851507</td>\n",
       "      <td>19</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61453075</td>\n",
       "      <td>merrily</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>merry</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dadj LY</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42274</td>\n",
       "      <td>16851507</td>\n",
       "      <td>19</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>62816244</td>\n",
       "      <td>largest</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>large</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SP</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22756</td>\n",
       "      <td>22755</td>\n",
       "      <td>42518</td>\n",
       "      <td>17177704</td>\n",
       "      <td>18</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>1</td>\n",
       "      <td>one of the largest snakes is the giant anacond...</td>\n",
       "      <td>adj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>61971711</td>\n",
       "      <td>picture's</td>\n",
       "      <td>eng</td>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td>picture</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22729</td>\n",
       "      <td>22728</td>\n",
       "      <td>42372</td>\n",
       "      <td>16979363</td>\n",
       "      <td>3</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0</td>\n",
       "      <td>okay let's put them all take them out and put ...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>61474947</td>\n",
       "      <td>onto</td>\n",
       "      <td>eng</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>onto</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42286</td>\n",
       "      <td>16859093</td>\n",
       "      <td>111</td>\n",
       "      <td>4.709530</td>\n",
       "      <td>2</td>\n",
       "      <td>it's a whole stack so Rusty's pushing Percy on...</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>62551990</td>\n",
       "      <td>dumping</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>dump</td>\n",
       "      <td>dʌmpiŋ</td>\n",
       "      <td>dʌmpɪŋ</td>\n",
       "      <td>PRESP</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22743</td>\n",
       "      <td>22743</td>\n",
       "      <td>42433</td>\n",
       "      <td>17094010</td>\n",
       "      <td>17</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>1</td>\n",
       "      <td>it's dumping some dirt</td>\n",
       "      <td>part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>62904807</td>\n",
       "      <td>mashed</td>\n",
       "      <td>eng</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>mash</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>PASTP</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22766</td>\n",
       "      <td>22764</td>\n",
       "      <td>42548</td>\n",
       "      <td>17198054</td>\n",
       "      <td>6</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1</td>\n",
       "      <td>wanna do mashed potato</td>\n",
       "      <td>part</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     token_id      gloss language  token_order prefix      stem  \\\n",
       "0    62438075       this      eng            1             this   \n",
       "1    61402650       need      eng            3             need   \n",
       "2    62051390   anything      eng            4         anything   \n",
       "3    61453077    merrily      eng            4            merry   \n",
       "4    61453075    merrily      eng            2            merry   \n",
       "..        ...        ...      ...          ...    ...       ...   \n",
       "995  62816244    largest      eng            4            large   \n",
       "996  61971711  picture's      eng           17          picture   \n",
       "997  61474947       onto      eng            9             onto   \n",
       "998  62551990    dumping      eng            2             dump   \n",
       "999  62904807     mashed      eng            3             mash   \n",
       "\n",
       "    actual_phonology model_phonology   suffix  num_morphemes  ... corpus_id  \\\n",
       "0                                                          1  ...       328   \n",
       "1                                                          1  ...       328   \n",
       "2                                                          1  ...       328   \n",
       "3                                     dadj LY              3  ...       328   \n",
       "4                                     dadj LY              3  ...       328   \n",
       "..               ...             ...      ...            ...  ...       ...   \n",
       "995                                        SP              2  ...       328   \n",
       "996                                                        2  ...       328   \n",
       "997                                                        1  ...       328   \n",
       "998           dʌmpiŋ          dʌmpɪŋ    PRESP              2  ...       328   \n",
       "999                                     PASTP              2  ...       328   \n",
       "\n",
       "    speaker_id target_child_id transcript_id utterance_id frequency  \\\n",
       "0        22744           22743         42448     17084600     10570   \n",
       "1        22707           22704         42252     16839687      2591   \n",
       "2        22729           22728         42378     16986849       407   \n",
       "3        22721           22720         42274     16851507        19   \n",
       "4        22721           22720         42274     16851507        19   \n",
       "..         ...             ...           ...          ...       ...   \n",
       "995      22756           22755         42518     17177704        18   \n",
       "996      22729           22728         42372     16979363         3   \n",
       "997      22721           22720         42286     16859093       111   \n",
       "998      22743           22743         42433     17094010        17   \n",
       "999      22766           22764         42548     17198054         6   \n",
       "\n",
       "    log_frequency log_frequency_bin  \\\n",
       "0        9.265775                 4   \n",
       "1        7.859799                 4   \n",
       "2        6.008813                 2   \n",
       "3        2.944439                 1   \n",
       "4        2.944439                 1   \n",
       "..            ...               ...   \n",
       "995      2.890372                 1   \n",
       "996      1.098612                 0   \n",
       "997      4.709530                 2   \n",
       "998      2.833213                 1   \n",
       "999      1.791759                 1   \n",
       "\n",
       "                                       utterance_gloss part_of_speech  \n",
       "0                  this one looks like she's driving a        pro:dem  \n",
       "1                                           now I need              v  \n",
       "2          did we see anything else when we were there      pro:indef  \n",
       "3                      merrily merrily merrily merrily            adv  \n",
       "4                      merrily merrily merrily merrily            adv  \n",
       "..                                                 ...            ...  \n",
       "995  one of the largest snakes is the giant anacond...            adj  \n",
       "996  okay let's put them all take them out and put ...              n  \n",
       "997  it's a whole stack so Rusty's pushing Percy on...           prep  \n",
       "998                             it's dumping some dirt           part  \n",
       "999                             wanna do mashed potato           part  \n",
       "\n",
       "[1000 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_full_tokens = pd.read_csv(FULL_SAMPLED_TOKENS_CSV_NAME, keep_default_na=False, index_col=0)\n",
    "sampled_full_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignment function\n",
    "\n",
    "def childes_to_tagger_mapping(utterance, tokenizer_info):\n",
    "    '''\n",
    "    utterance: utterance as recorded in childesdb (contains spaces, underscores, plus signs, apostrophes, etc.)\n",
    "    returns: dictionary mapping (token index in utterance, token) to a list of indices that the token maps to in the tagger's tokenizer\n",
    "    '''\n",
    "\n",
    "    # map each childes token (by token index) into a list of indices (character indices)\n",
    "    char_indices_of_token = dict()\n",
    "    char_index = 0\n",
    "    for index, token in enumerate(utterance.split()):\n",
    "        token_char_length = len(token) - token.count(\"_\") - token.count(\"+\")\n",
    "        char_indices_of_token[(index, token)] = [i for i in range(char_index, char_index + token_char_length)]\n",
    "        char_index += token_char_length\n",
    "        # len(token) gets complicated when we remove _ and + from utterance\n",
    "\n",
    "    # map each index to a token in the tokenizer\n",
    "    char_index_to_tokenizer = dict()\n",
    "    char_index = 0\n",
    "    for token_index in range(len(tokenizer_info)):\n",
    "        token = tokenizer_info[token_index]['text']\n",
    "        for i in range(char_index, char_index + len(token)):\n",
    "            char_index_to_tokenizer[i] = token_index\n",
    "        char_index += len(token)\n",
    "  \n",
    "    # map token in utterance to token in tokenizer using char_indices_of_token and char_index_to_tokenizer\n",
    "    utterance_to_tokenizer_index = dict()\n",
    "    for index, token in enumerate(utterance.split()):\n",
    "        tokenizer_tokens = set()\n",
    "        for i in char_indices_of_token[(index, token)]:\n",
    "            tokenizer_tokens.add(char_index_to_tokenizer[i])\n",
    "        tokenizer_tokens = list(tokenizer_tokens)\n",
    "        tokenizer_tokens.sort()\n",
    "        # utterance_to_tokenizer_index[(index, token)] = tokenizer_tokens\n",
    "        utterance_to_tokenizer_index[index] = tokenizer_tokens\n",
    "\n",
    "    return utterance_to_tokenizer_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger_to_childes_mapping(utterance, tokenizer_info):\n",
    "    '''\n",
    "    utterance: utterance as recorded in childesdb (contains spaces, underscores, plus signs, apostrophes, etc.)\n",
    "    returns: dictionary mapping (token index in tagger's tokenizer, token) to a list of indices that the token maps to in the childesdb utterance\n",
    "    '''\n",
    "\n",
    "    # map each tagger token (by token index) into a list of character indices\n",
    "    tagger_to_index = dict()\n",
    "    char_index = 0\n",
    "    for token_index in range(len(tokenizer_info)):\n",
    "        token = tokenizer_info[token_index].text\n",
    "        tagger_to_index[(token_index, token)] = [i for i in range(char_index, char_index + len(token))]\n",
    "        char_index += len(token)\n",
    "\n",
    "    # map each character index to a token in the original childes utterance\n",
    "    index_to_childes = dict()\n",
    "    char_index = 0\n",
    "    for index, token in enumerate(utterance.split()):\n",
    "        token_char_length = len(token) - token.count(\"_\") - token.count(\"+\")\n",
    "        for i in range(char_index, char_index + token_char_length):\n",
    "            index_to_childes[i] = index\n",
    "        char_index += token_char_length\n",
    "\n",
    "    # map tagger token to childes token\n",
    "    tagger_to_childes = dict()\n",
    "    for token_index in range(len(tokenizer_info)):\n",
    "        token = tokenizer_info[token_index]['text']\n",
    "        indices = set()\n",
    "        for char_index in tagger_to_index[(token_index, token)]:\n",
    "            indices.add(index_to_childes[char_index])\n",
    "        indices = list(indices)\n",
    "        indices.sort()\n",
    "        # tagger_to_childes[(token_index, token)] = indices\n",
    "        tagger_to_childes[token_index] = indices\n",
    "    \n",
    "    return tagger_to_childes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>language</th>\n",
       "      <th>token_order</th>\n",
       "      <th>prefix</th>\n",
       "      <th>stem</th>\n",
       "      <th>actual_phonology</th>\n",
       "      <th>model_phonology</th>\n",
       "      <th>suffix</th>\n",
       "      <th>num_morphemes</th>\n",
       "      <th>...</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>target_child_id</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>log_frequency</th>\n",
       "      <th>log_frequency_bin</th>\n",
       "      <th>utterance_gloss</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62438075</td>\n",
       "      <td>this</td>\n",
       "      <td>eng</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22744</td>\n",
       "      <td>22743</td>\n",
       "      <td>42448</td>\n",
       "      <td>17084600</td>\n",
       "      <td>10570</td>\n",
       "      <td>9.265775</td>\n",
       "      <td>4</td>\n",
       "      <td>this one looks like she's driving a</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61402650</td>\n",
       "      <td>need</td>\n",
       "      <td>eng</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>need</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22707</td>\n",
       "      <td>22704</td>\n",
       "      <td>42252</td>\n",
       "      <td>16839687</td>\n",
       "      <td>2591</td>\n",
       "      <td>7.859799</td>\n",
       "      <td>4</td>\n",
       "      <td>now I need</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62051390</td>\n",
       "      <td>anything</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>anything</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22729</td>\n",
       "      <td>22728</td>\n",
       "      <td>42378</td>\n",
       "      <td>16986849</td>\n",
       "      <td>407</td>\n",
       "      <td>6.008813</td>\n",
       "      <td>2</td>\n",
       "      <td>did we see anything else when we were there</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61453077</td>\n",
       "      <td>merrily</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>merry</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dadj LY</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42274</td>\n",
       "      <td>16851507</td>\n",
       "      <td>19</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61453075</td>\n",
       "      <td>merrily</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>merry</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dadj LY</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42274</td>\n",
       "      <td>16851507</td>\n",
       "      <td>19</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>62816244</td>\n",
       "      <td>largest</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>large</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SP</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22756</td>\n",
       "      <td>22755</td>\n",
       "      <td>42518</td>\n",
       "      <td>17177704</td>\n",
       "      <td>18</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>1</td>\n",
       "      <td>one of the largest snakes is the giant anacond...</td>\n",
       "      <td>adj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>61971711</td>\n",
       "      <td>picture_'s</td>\n",
       "      <td>eng</td>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td>picture</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22729</td>\n",
       "      <td>22728</td>\n",
       "      <td>42372</td>\n",
       "      <td>16979363</td>\n",
       "      <td>3</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0</td>\n",
       "      <td>okay let's put them all take them out and put ...</td>\n",
       "      <td>n+cop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>61474947</td>\n",
       "      <td>onto</td>\n",
       "      <td>eng</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>onto</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42286</td>\n",
       "      <td>16859093</td>\n",
       "      <td>111</td>\n",
       "      <td>4.709530</td>\n",
       "      <td>2</td>\n",
       "      <td>it's a whole stack so Rusty's pushing Percy on...</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>62551990</td>\n",
       "      <td>dumping</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>dump</td>\n",
       "      <td>dʌmpiŋ</td>\n",
       "      <td>dʌmpɪŋ</td>\n",
       "      <td>PRESP</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22743</td>\n",
       "      <td>22743</td>\n",
       "      <td>42433</td>\n",
       "      <td>17094010</td>\n",
       "      <td>17</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>1</td>\n",
       "      <td>it's dumping some dirt</td>\n",
       "      <td>part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>62904807</td>\n",
       "      <td>mashed</td>\n",
       "      <td>eng</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>mash</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>PASTP</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>22766</td>\n",
       "      <td>22764</td>\n",
       "      <td>42548</td>\n",
       "      <td>17198054</td>\n",
       "      <td>6</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1</td>\n",
       "      <td>wanna do mashed potato</td>\n",
       "      <td>part</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     token_id       gloss language  token_order prefix      stem  \\\n",
       "0    62438075        this      eng            1             this   \n",
       "1    61402650        need      eng            3             need   \n",
       "2    62051390    anything      eng            4         anything   \n",
       "3    61453077     merrily      eng            4            merry   \n",
       "4    61453075     merrily      eng            2            merry   \n",
       "..        ...         ...      ...          ...    ...       ...   \n",
       "995  62816244     largest      eng            4            large   \n",
       "996  61971711  picture_'s      eng           17          picture   \n",
       "997  61474947        onto      eng            9             onto   \n",
       "998  62551990     dumping      eng            2             dump   \n",
       "999  62904807      mashed      eng            3             mash   \n",
       "\n",
       "    actual_phonology model_phonology   suffix  num_morphemes  ... corpus_id  \\\n",
       "0                                                          1  ...       328   \n",
       "1                                                          1  ...       328   \n",
       "2                                                          1  ...       328   \n",
       "3                                     dadj LY              3  ...       328   \n",
       "4                                     dadj LY              3  ...       328   \n",
       "..               ...             ...      ...            ...  ...       ...   \n",
       "995                                        SP              2  ...       328   \n",
       "996                                                        2  ...       328   \n",
       "997                                                        1  ...       328   \n",
       "998           dʌmpiŋ          dʌmpɪŋ    PRESP              2  ...       328   \n",
       "999                                     PASTP              2  ...       328   \n",
       "\n",
       "    speaker_id target_child_id transcript_id utterance_id frequency  \\\n",
       "0        22744           22743         42448     17084600     10570   \n",
       "1        22707           22704         42252     16839687      2591   \n",
       "2        22729           22728         42378     16986849       407   \n",
       "3        22721           22720         42274     16851507        19   \n",
       "4        22721           22720         42274     16851507        19   \n",
       "..         ...             ...           ...          ...       ...   \n",
       "995      22756           22755         42518     17177704        18   \n",
       "996      22729           22728         42372     16979363         3   \n",
       "997      22721           22720         42286     16859093       111   \n",
       "998      22743           22743         42433     17094010        17   \n",
       "999      22766           22764         42548     17198054         6   \n",
       "\n",
       "    log_frequency log_frequency_bin  \\\n",
       "0        9.265775                 4   \n",
       "1        7.859799                 4   \n",
       "2        6.008813                 2   \n",
       "3        2.944439                 1   \n",
       "4        2.944439                 1   \n",
       "..            ...               ...   \n",
       "995      2.890372                 1   \n",
       "996      1.098612                 0   \n",
       "997      4.709530                 2   \n",
       "998      2.833213                 1   \n",
       "999      1.791759                 1   \n",
       "\n",
       "                                       utterance_gloss part_of_speech  \n",
       "0                  this one looks like she's driving a            pro  \n",
       "1                                           now I need              v  \n",
       "2          did we see anything else when we were there            pro  \n",
       "3                      merrily merrily merrily merrily            adv  \n",
       "4                      merrily merrily merrily merrily            adv  \n",
       "..                                                 ...            ...  \n",
       "995  one of the largest snakes is the giant anacond...            adj  \n",
       "996  okay let's put them all take them out and put ...          n+cop  \n",
       "997  it's a whole stack so Rusty's pushing Percy on...           prep  \n",
       "998                             it's dumping some dirt           part  \n",
       "999                             wanna do mashed potato           part  \n",
       "\n",
       "[1000 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if clitic exists, take first of POS \"+\" first of clitic\n",
    "  # create new POS category for contractions\n",
    "token_copy = sampled_full_tokens.copy()\n",
    "\n",
    "# take the general category, i.e. anything before the colon, except for dets\n",
    "def simplify_pos(pos): return pos[:pos.find(\":\")] if (pos.find(\":\") != -1 and pos.find(\" \") == -1 and pos[:pos.find(\":\")] != \"det\") else pos\n",
    "\n",
    "# determine if it's a contraction\n",
    "def contraction_pos(x):\n",
    "    part_of_speech, clitic, suffix = x['part_of_speech'], x['clitic'], x['suffix']\n",
    "    gloss = x['gloss']\n",
    "    utterance = x['utterance_gloss']\n",
    "    token_order = x['token_order']\n",
    "    info = get_nlp_tokenizer(utterance)\n",
    "    alignment = childes_to_tagger_mapping(utterance, info)\n",
    "    if len(alignment[token_order - 1]) > 1:\n",
    "        num_subtokens = len(alignment[token_order - 1])\n",
    "        fake_gloss = \"_\".join([info[tagger_tok_ind]['text'] for tagger_tok_ind in alignment[token_order - 1]])\n",
    "        # determine part of speech\n",
    "        # this is somewhat english-centric (mostly for suffix), not sure how to get around that\n",
    "        if \"POSS\" in suffix:\n",
    "            fake_part_of_speech = (part_of_speech + \"+\")*(num_subtokens - 1) + 'poss'\n",
    "        elif clitic != \"\":\n",
    "            fake_part_of_speech = (part_of_speech + \"+\")*(num_subtokens - 1) + clitic.split()[0]\n",
    "        else:\n",
    "            fake_part_of_speech = (part_of_speech + \"+\")*(num_subtokens - 1) + part_of_speech\n",
    "        return (fake_gloss, fake_part_of_speech)\n",
    "    return (gloss, part_of_speech)\n",
    "\n",
    "token_copy['part_of_speech'] = token_copy['part_of_speech'].map(simplify_pos)\n",
    "new = token_copy.apply(lambda x: contraction_pos(x), axis=1, result_type=\"expand\")\n",
    "token_copy['gloss'] = new[0]\n",
    "token_copy['part_of_speech'] = new[1]\n",
    "token_copy.drop(token_copy.columns[token_copy.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "token_copy\n",
    "\n",
    "# token_copy['gloss'], token_copy['part_of_speech'] = token_copy.apply(lambda x: contraction_pos(x), axis=1)\n",
    "# print(token_copy[[\"gloss\", \"part_of_speech\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>language</th>\n",
       "      <th>token_order</th>\n",
       "      <th>prefix</th>\n",
       "      <th>stem</th>\n",
       "      <th>actual_phonology</th>\n",
       "      <th>model_phonology</th>\n",
       "      <th>suffix</th>\n",
       "      <th>num_morphemes</th>\n",
       "      <th>...</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>target_child_id</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>log_frequency</th>\n",
       "      <th>log_frequency_bin</th>\n",
       "      <th>utterance_gloss</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>split_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62438075</td>\n",
       "      <td>this</td>\n",
       "      <td>eng</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>22744</td>\n",
       "      <td>22743</td>\n",
       "      <td>42448</td>\n",
       "      <td>17084600</td>\n",
       "      <td>10570</td>\n",
       "      <td>9.265775</td>\n",
       "      <td>4</td>\n",
       "      <td>this one looks like she's driving a</td>\n",
       "      <td>pro</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61402650</td>\n",
       "      <td>need</td>\n",
       "      <td>eng</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>need</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>22707</td>\n",
       "      <td>22704</td>\n",
       "      <td>42252</td>\n",
       "      <td>16839687</td>\n",
       "      <td>2591</td>\n",
       "      <td>7.859799</td>\n",
       "      <td>4</td>\n",
       "      <td>now I need</td>\n",
       "      <td>v</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62051390</td>\n",
       "      <td>anything</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>anything</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>22729</td>\n",
       "      <td>22728</td>\n",
       "      <td>42378</td>\n",
       "      <td>16986849</td>\n",
       "      <td>407</td>\n",
       "      <td>6.008813</td>\n",
       "      <td>2</td>\n",
       "      <td>did we see anything else when we were there</td>\n",
       "      <td>pro</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61453077</td>\n",
       "      <td>merrily</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>merry</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dadj LY</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42274</td>\n",
       "      <td>16851507</td>\n",
       "      <td>19</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61453075</td>\n",
       "      <td>merrily</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>merry</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dadj LY</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42274</td>\n",
       "      <td>16851507</td>\n",
       "      <td>19</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>61643679</td>\n",
       "      <td>'s</td>\n",
       "      <td>eng</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>nephew</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dn POSS</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>22729</td>\n",
       "      <td>22728</td>\n",
       "      <td>42318</td>\n",
       "      <td>16904905</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0</td>\n",
       "      <td>my niece's birthday was Monday and my nephew's...</td>\n",
       "      <td>poss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>61529288</td>\n",
       "      <td>Noah</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Noah</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dn POSS</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42284</td>\n",
       "      <td>16873728</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0</td>\n",
       "      <td>the Noah's ark you have yeah</td>\n",
       "      <td>adj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>61529288</td>\n",
       "      <td>'s</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Noah</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dn POSS</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>22721</td>\n",
       "      <td>22720</td>\n",
       "      <td>42284</td>\n",
       "      <td>16873728</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0</td>\n",
       "      <td>the Noah's ark you have yeah</td>\n",
       "      <td>poss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>61971711</td>\n",
       "      <td>picture</td>\n",
       "      <td>eng</td>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td>picture</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>22729</td>\n",
       "      <td>22728</td>\n",
       "      <td>42372</td>\n",
       "      <td>16979363</td>\n",
       "      <td>3</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0</td>\n",
       "      <td>okay let's put them all take them out and put ...</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>61971711</td>\n",
       "      <td>'s</td>\n",
       "      <td>eng</td>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td>picture</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>22729</td>\n",
       "      <td>22728</td>\n",
       "      <td>42372</td>\n",
       "      <td>16979363</td>\n",
       "      <td>3</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0</td>\n",
       "      <td>okay let's put them all take them out and put ...</td>\n",
       "      <td>cop</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_id     gloss language  token_order prefix      stem  \\\n",
       "0     62438075      this      eng            1             this   \n",
       "1     61402650      need      eng            3             need   \n",
       "2     62051390  anything      eng            4         anything   \n",
       "3     61453077   merrily      eng            4            merry   \n",
       "4     61453075   merrily      eng            2            merry   \n",
       "...        ...       ...      ...          ...    ...       ...   \n",
       "1097  61643679        's      eng            8           nephew   \n",
       "1098  61529288      Noah      eng            2             Noah   \n",
       "1099  61529288        's      eng            2             Noah   \n",
       "1100  61971711   picture      eng           17          picture   \n",
       "1101  61971711        's      eng           17          picture   \n",
       "\n",
       "     actual_phonology model_phonology   suffix  num_morphemes  ... speaker_id  \\\n",
       "0                                                           1  ...      22744   \n",
       "1                                                           1  ...      22707   \n",
       "2                                                           1  ...      22729   \n",
       "3                                      dadj LY              3  ...      22721   \n",
       "4                                      dadj LY              3  ...      22721   \n",
       "...               ...             ...      ...            ...  ...        ...   \n",
       "1097                                   dn POSS              3  ...      22729   \n",
       "1098                                   dn POSS              3  ...      22721   \n",
       "1099                                   dn POSS              3  ...      22721   \n",
       "1100                                                        2  ...      22729   \n",
       "1101                                                        2  ...      22729   \n",
       "\n",
       "     target_child_id transcript_id utterance_id frequency log_frequency  \\\n",
       "0              22743         42448     17084600     10570      9.265775   \n",
       "1              22704         42252     16839687      2591      7.859799   \n",
       "2              22728         42378     16986849       407      6.008813   \n",
       "3              22720         42274     16851507        19      2.944439   \n",
       "4              22720         42274     16851507        19      2.944439   \n",
       "...              ...           ...          ...       ...           ...   \n",
       "1097           22728         42318     16904905         2      0.693147   \n",
       "1098           22720         42284     16873728         2      0.693147   \n",
       "1099           22720         42284     16873728         2      0.693147   \n",
       "1100           22728         42372     16979363         3      1.098612   \n",
       "1101           22728         42372     16979363         3      1.098612   \n",
       "\n",
       "     log_frequency_bin                                    utterance_gloss  \\\n",
       "0                    4                this one looks like she's driving a   \n",
       "1                    4                                         now I need   \n",
       "2                    2        did we see anything else when we were there   \n",
       "3                    1                    merrily merrily merrily merrily   \n",
       "4                    1                    merrily merrily merrily merrily   \n",
       "...                ...                                                ...   \n",
       "1097                 0  my niece's birthday was Monday and my nephew's...   \n",
       "1098                 0                       the Noah's ark you have yeah   \n",
       "1099                 0                       the Noah's ark you have yeah   \n",
       "1100                 0  okay let's put them all take them out and put ...   \n",
       "1101                 0  okay let's put them all take them out and put ...   \n",
       "\n",
       "      part_of_speech split_index  \n",
       "0                pro           0  \n",
       "1                  v           0  \n",
       "2                pro           0  \n",
       "3                adv           0  \n",
       "4                adv           0  \n",
       "...              ...         ...  \n",
       "1097            poss           1  \n",
       "1098             adj           0  \n",
       "1099            poss           1  \n",
       "1100               n           0  \n",
       "1101             cop           1  \n",
       "\n",
       "[1102 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thousand_samples = token_copy.copy()\n",
    "thousand_samples['split_index'] = 0\n",
    "\n",
    "def is_contraction(gloss, pos):\n",
    "    if pos.find(\"+\") != -1:\n",
    "        return True\n",
    "    if gloss.find(\"_\") != -1:\n",
    "        return True\n",
    "    if gloss.find(\"+\") != -1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def split_word(gloss):\n",
    "    if gloss.find(\"_\") != -1:\n",
    "        return gloss.split(\"_\")\n",
    "    if gloss.find(\"+\") != -1:\n",
    "        return gloss.split(\"+\")\n",
    "    if gloss.find(\" \") != -1:\n",
    "        return gloss.split(\" \")\n",
    "    print(gloss)\n",
    "\n",
    "# split contractions into two lines\n",
    "split_contractions = pd.DataFrame()\n",
    "# save the original one-line contraction to delete\n",
    "contractions_to_delete = pd.DataFrame()\n",
    "\n",
    "for index,row in thousand_samples.iterrows():\n",
    "    gloss = row['gloss']\n",
    "    pos = row['part_of_speech']\n",
    "    if not is_contraction(gloss, pos): \n",
    "        continue\n",
    "\n",
    "    # otherwise, it is a contraction\n",
    "    split_gloss = split_word(gloss)\n",
    "    new_rows = [row.copy() for i in range(len(split_gloss))]\n",
    "    for i in range(len(split_gloss)):\n",
    "        new_rows[i]['split_index'] += i\n",
    "        new_rows[i]['gloss'] = split_gloss[i]\n",
    "        new_rows[i]['part_of_speech'] = pos.split(\"+\")[i]\n",
    "    split_contractions = split_contractions.append(new_rows)\n",
    "    contractions_to_delete = contractions_to_delete.append([row])\n",
    "\n",
    "samples_without_contractions = pd.concat([thousand_samples, contractions_to_delete]).drop_duplicates(keep=False)\n",
    "samples_with_split_contractions = samples_without_contractions.append(split_contractions).reset_index()\n",
    "\n",
    "final_samples = samples_with_split_contractions.drop(columns=['index'])\n",
    "final_samples\n",
    "\n",
    "#final_samples = samples_with_split_contractions.filter(['id', 'token_id', 'gloss', \"utterance_id\", 'utterance_gloss', \"part_of_speech\", \"log_frequency_bin\", \"speaker_role\", \"token_order\", \"split_index\"])\n",
    "#final_samples.rename(columns = {'id':'token_id'}, inplace = True) \n",
    "#print(final_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>language</th>\n",
       "      <th>token_order</th>\n",
       "      <th>prefix</th>\n",
       "      <th>stem</th>\n",
       "      <th>actual_phonology</th>\n",
       "      <th>model_phonology</th>\n",
       "      <th>suffix</th>\n",
       "      <th>num_morphemes</th>\n",
       "      <th>...</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>frequency</th>\n",
       "      <th>log_frequency</th>\n",
       "      <th>log_frequency_bin</th>\n",
       "      <th>utterance_gloss</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>split_index</th>\n",
       "      <th>tagger_part_of_speech</th>\n",
       "      <th>tagger_morphology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62438075</td>\n",
       "      <td>this</td>\n",
       "      <td>eng</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>this</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>42448</td>\n",
       "      <td>17084600</td>\n",
       "      <td>10570</td>\n",
       "      <td>9.265775</td>\n",
       "      <td>4</td>\n",
       "      <td>this one looks like she's driving a</td>\n",
       "      <td>pro</td>\n",
       "      <td>0</td>\n",
       "      <td>DET</td>\n",
       "      <td>Number=Sing|PronType=Dem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61402650</td>\n",
       "      <td>need</td>\n",
       "      <td>eng</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>need</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>42252</td>\n",
       "      <td>16839687</td>\n",
       "      <td>2591</td>\n",
       "      <td>7.859799</td>\n",
       "      <td>4</td>\n",
       "      <td>now I need</td>\n",
       "      <td>v</td>\n",
       "      <td>0</td>\n",
       "      <td>VERB</td>\n",
       "      <td>Tense=Pres|VerbForm=Fin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62051390</td>\n",
       "      <td>anything</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>anything</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>42378</td>\n",
       "      <td>16986849</td>\n",
       "      <td>407</td>\n",
       "      <td>6.008813</td>\n",
       "      <td>2</td>\n",
       "      <td>did we see anything else when we were there</td>\n",
       "      <td>pro</td>\n",
       "      <td>0</td>\n",
       "      <td>PRON</td>\n",
       "      <td>Number=Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61453077</td>\n",
       "      <td>merrily</td>\n",
       "      <td>eng</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>merry</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dadj LY</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>42274</td>\n",
       "      <td>16851507</td>\n",
       "      <td>19</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "      <td>0</td>\n",
       "      <td>ADV</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61453075</td>\n",
       "      <td>merrily</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>merry</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dadj LY</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>42274</td>\n",
       "      <td>16851507</td>\n",
       "      <td>19</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "      <td>0</td>\n",
       "      <td>ADV</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>61643679</td>\n",
       "      <td>'s</td>\n",
       "      <td>eng</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>nephew</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dn POSS</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>42318</td>\n",
       "      <td>16904905</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0</td>\n",
       "      <td>my niece's birthday was Monday and my nephew's...</td>\n",
       "      <td>poss</td>\n",
       "      <td>1</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>61529288</td>\n",
       "      <td>Noah</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Noah</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dn POSS</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>42284</td>\n",
       "      <td>16873728</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0</td>\n",
       "      <td>the Noah's ark you have yeah</td>\n",
       "      <td>adj</td>\n",
       "      <td>0</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NounType=Prop|Number=Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>61529288</td>\n",
       "      <td>'s</td>\n",
       "      <td>eng</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Noah</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>dn POSS</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>42284</td>\n",
       "      <td>16873728</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0</td>\n",
       "      <td>the Noah's ark you have yeah</td>\n",
       "      <td>poss</td>\n",
       "      <td>1</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>61971711</td>\n",
       "      <td>picture</td>\n",
       "      <td>eng</td>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td>picture</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>42372</td>\n",
       "      <td>16979363</td>\n",
       "      <td>3</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0</td>\n",
       "      <td>okay let's put them all take them out and put ...</td>\n",
       "      <td>n</td>\n",
       "      <td>0</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Number=Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>61971711</td>\n",
       "      <td>'s</td>\n",
       "      <td>eng</td>\n",
       "      <td>17</td>\n",
       "      <td></td>\n",
       "      <td>picture</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>42372</td>\n",
       "      <td>16979363</td>\n",
       "      <td>3</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0</td>\n",
       "      <td>okay let's put them all take them out and put ...</td>\n",
       "      <td>cop</td>\n",
       "      <td>1</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_id     gloss language  token_order prefix      stem  \\\n",
       "0     62438075      this      eng            1             this   \n",
       "1     61402650      need      eng            3             need   \n",
       "2     62051390  anything      eng            4         anything   \n",
       "3     61453077   merrily      eng            4            merry   \n",
       "4     61453075   merrily      eng            2            merry   \n",
       "...        ...       ...      ...          ...    ...       ...   \n",
       "1097  61643679        's      eng            8           nephew   \n",
       "1098  61529288      Noah      eng            2             Noah   \n",
       "1099  61529288        's      eng            2             Noah   \n",
       "1100  61971711   picture      eng           17          picture   \n",
       "1101  61971711        's      eng           17          picture   \n",
       "\n",
       "     actual_phonology model_phonology   suffix  num_morphemes  ...  \\\n",
       "0                                                           1  ...   \n",
       "1                                                           1  ...   \n",
       "2                                                           1  ...   \n",
       "3                                      dadj LY              3  ...   \n",
       "4                                      dadj LY              3  ...   \n",
       "...               ...             ...      ...            ...  ...   \n",
       "1097                                   dn POSS              3  ...   \n",
       "1098                                   dn POSS              3  ...   \n",
       "1099                                   dn POSS              3  ...   \n",
       "1100                                                        2  ...   \n",
       "1101                                                        2  ...   \n",
       "\n",
       "     transcript_id utterance_id frequency log_frequency log_frequency_bin  \\\n",
       "0            42448     17084600     10570      9.265775                 4   \n",
       "1            42252     16839687      2591      7.859799                 4   \n",
       "2            42378     16986849       407      6.008813                 2   \n",
       "3            42274     16851507        19      2.944439                 1   \n",
       "4            42274     16851507        19      2.944439                 1   \n",
       "...            ...          ...       ...           ...               ...   \n",
       "1097         42318     16904905         2      0.693147                 0   \n",
       "1098         42284     16873728         2      0.693147                 0   \n",
       "1099         42284     16873728         2      0.693147                 0   \n",
       "1100         42372     16979363         3      1.098612                 0   \n",
       "1101         42372     16979363         3      1.098612                 0   \n",
       "\n",
       "                                        utterance_gloss part_of_speech  \\\n",
       "0                   this one looks like she's driving a            pro   \n",
       "1                                            now I need              v   \n",
       "2           did we see anything else when we were there            pro   \n",
       "3                       merrily merrily merrily merrily            adv   \n",
       "4                       merrily merrily merrily merrily            adv   \n",
       "...                                                 ...            ...   \n",
       "1097  my niece's birthday was Monday and my nephew's...           poss   \n",
       "1098                       the Noah's ark you have yeah            adj   \n",
       "1099                       the Noah's ark you have yeah           poss   \n",
       "1100  okay let's put them all take them out and put ...              n   \n",
       "1101  okay let's put them all take them out and put ...            cop   \n",
       "\n",
       "     split_index  tagger_part_of_speech          tagger_morphology  \n",
       "0              0                    DET   Number=Sing|PronType=Dem  \n",
       "1              0                   VERB    Tense=Pres|VerbForm=Fin  \n",
       "2              0                   PRON                Number=Sing  \n",
       "3              0                    ADV                             \n",
       "4              0                    ADV                             \n",
       "...          ...                    ...                        ...  \n",
       "1097           1                   PART                             \n",
       "1098           0                  PROPN  NounType=Prop|Number=Sing  \n",
       "1099           1                   PART                             \n",
       "1100           0                   NOUN                Number=Sing  \n",
       "1101           1                   PART                             \n",
       "\n",
       "[1102 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a tagger_part_of_speech and tagger_morphology column\n",
    "\n",
    "\"\"\"\n",
    "def tagger_pos(token, utterance, token_order, split_index):\n",
    "  '''\n",
    "  note that token_order is one-indexed\n",
    "  '''\n",
    "  info = get_nlp_tokenizer(utterance)\n",
    "  alignment = childes_to_tagger_mapping(utterance, info)\n",
    "  if token_order - 1 in alignment:\n",
    "    tokenizer_index = alignment[token_order - 1]\n",
    "    if split_index >= len(tokenizer_index):\n",
    "        print(token)\n",
    "        print(utterance)\n",
    "        print(token_order)\n",
    "        print(split_index)\n",
    "        print(alignment)\n",
    "    return info[tokenizer_index[split_index]].pos_\n",
    "\n",
    "  # otherwise, the alignment didn't work or the token maps to multiple tagger categories\n",
    "  print(\"1\")\n",
    "  print(token, token_order)\n",
    "  print(utterance)\n",
    "  print(str(alignment))\n",
    "  info = nlp(token)\n",
    "  return info[0].pos_\n",
    "\n",
    "def tagger_morph(token, utterance, token_order, split_index):\n",
    "  info = get_nlp_tokenizer(utterance)\n",
    "  alignment = childes_to_tagger_mapping(utterance, info)\n",
    "  if token_order - 1 in alignment:\n",
    "    tokenizer_index = alignment[token_order - 1]\n",
    "    return \"|\".join(info[tokenizer_index[split_index]].morph)\n",
    "\n",
    "  # otherwise, the alignment didn't work or the token maps to multiple tagger categories\n",
    "  print(\"2\")\n",
    "  print(token, token_order)\n",
    "  print(utterance)\n",
    "  print(str(alignment))\n",
    "  info = nlp(token)\n",
    "  return \"|\".join(info[0].morph)\n",
    "\"\"\"\n",
    "\n",
    "def tagger_pos_clean(token, utterance, token_order, split_index):\n",
    "    '''\n",
    "    note that token_order is one-indexed\n",
    "    \"_clean\" functions remove xxx and yyy from utterance before passing into tagger\n",
    "    '''\n",
    "    utt_clean = \" \".join(utterance.replace(\"xxx\", \"\").replace(\"yyy\", \"\").split())\n",
    "    # ASSUMPTION FOR PROVIDENCE AND PARIS (NOT NECESSARILY TRUE FOR OTHER CORPUSES): NO CHILDES TOKEN HAS A SPACE IN IT\n",
    "    # new token order is the original minus the number of xxx's and yyy's BEFORE the token_order-th word in original utt\n",
    "    childes_tokens_before = utterance.split()[:token_order]\n",
    "    tokord_clean = token_order - childes_tokens_before.count(\"xxx\") - childes_tokens_before.count(\"yyy\")\n",
    "    info = get_nlp_tokenizer(utt_clean)\n",
    "    alignment = childes_to_tagger_mapping(utt_clean, info)\n",
    "    if tokord_clean - 1 in alignment:\n",
    "        tokenizer_index = alignment[tokord_clean - 1]\n",
    "        if split_index >= len(tokenizer_index):\n",
    "            print(\"not enough subtokens\")\n",
    "            print(token)\n",
    "            print(utterance)\n",
    "            print(token_order)\n",
    "            print(split_index)\n",
    "            print(alignment)\n",
    "        if tokenizer_index[split_index] >= len(info):\n",
    "            print(\"not enough tagger tokens\")\n",
    "            print(token)\n",
    "            print(utterance)\n",
    "            print(token_order)\n",
    "            print(split_index)\n",
    "            print(alignment)\n",
    "        return info[tokenizer_index[split_index]]['pos']\n",
    "\n",
    "    # otherwise, the alignment didn't work or the token maps to multiple tagger categories\n",
    "    print(token, token_order)\n",
    "    print(utterance)\n",
    "    print(str(alignment))\n",
    "    info = nlp(token)\n",
    "    return info[0]['pos']\n",
    "\n",
    "def tagger_morph_clean(token, utterance, token_order, split_index):\n",
    "    utt_clean = \" \".join(utterance.replace(\"xxx\", \"\").replace(\"yyy\", \"\").split())\n",
    "    # ASSUMPTION FOR PROVIDENCE (NOT NECESSARILY TRUE FOR OTHER CORPUSES): NO CHILDES TOKEN HAS A SPACE IN IT\n",
    "    # new token order is the original minus the number of xxx's and yyy's BEFORE the token_order-th word in original utt\n",
    "    childes_tokens_before = utterance.split()[:token_order]\n",
    "    tokord_clean = token_order - childes_tokens_before.count(\"xxx\") - childes_tokens_before.count(\"yyy\")\n",
    "    info = get_nlp_tokenizer(utt_clean)\n",
    "    alignment = childes_to_tagger_mapping(utt_clean, info)\n",
    "    if tokord_clean - 1 in alignment:\n",
    "        tokenizer_index = alignment[tokord_clean - 1]\n",
    "        if split_index >= len(tokenizer_index):\n",
    "            print(\"not enough subtokens\")\n",
    "            print(token)\n",
    "            print(utterance)\n",
    "            print(token_order)\n",
    "            print(split_index)\n",
    "            print(alignment)\n",
    "        if tokenizer_index[split_index] >= len(info):\n",
    "            print(\"not enough tagger tokens\")\n",
    "            print(token)\n",
    "            print(utterance)\n",
    "            print(token_order)\n",
    "            print(split_index)\n",
    "            print(alignment)\n",
    "        return info[tokenizer_index[split_index]]['morph']\n",
    "\n",
    "    # otherwise, the alignment didn't work or the token maps to multiple tagger categories\n",
    "    print(token, token_order)\n",
    "    print(utterance)\n",
    "    print(str(alignment))\n",
    "    info = nlp(token)\n",
    "    return info[0]['morph']\n",
    "\n",
    "final_samples[\"tagger_part_of_speech\"] = final_samples.apply(lambda x: tagger_pos_clean(x['gloss'], x['utterance_gloss'], x['token_order'], x['split_index']), axis=1)\n",
    "final_samples[\"tagger_morphology\"] = final_samples.apply(lambda x: tagger_morph_clean(x['gloss'], x['utterance_gloss'], x['token_order'], x['split_index']), axis=1)\n",
    "\n",
    "final_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>gloss</th>\n",
       "      <th>token_order</th>\n",
       "      <th>speaker_role</th>\n",
       "      <th>target_child_age</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>log_frequency_bin</th>\n",
       "      <th>utterance_gloss</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>tagger_part_of_speech</th>\n",
       "      <th>tagger_morphology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62438075</td>\n",
       "      <td>this</td>\n",
       "      <td>1</td>\n",
       "      <td>Mother</td>\n",
       "      <td>29.624838</td>\n",
       "      <td>17084600</td>\n",
       "      <td>4</td>\n",
       "      <td>this one looks like she's driving a</td>\n",
       "      <td>pro</td>\n",
       "      <td>DET</td>\n",
       "      <td>Number=Sing|PronType=Dem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61402650</td>\n",
       "      <td>need</td>\n",
       "      <td>3</td>\n",
       "      <td>Mother</td>\n",
       "      <td>40.329370</td>\n",
       "      <td>16839687</td>\n",
       "      <td>4</td>\n",
       "      <td>now I need</td>\n",
       "      <td>v</td>\n",
       "      <td>VERB</td>\n",
       "      <td>Tense=Pres|VerbForm=Fin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62051390</td>\n",
       "      <td>anything</td>\n",
       "      <td>4</td>\n",
       "      <td>Mother</td>\n",
       "      <td>41.920779</td>\n",
       "      <td>16986849</td>\n",
       "      <td>2</td>\n",
       "      <td>did we see anything else when we were there</td>\n",
       "      <td>pro</td>\n",
       "      <td>PRON</td>\n",
       "      <td>Number=Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61453077</td>\n",
       "      <td>merrily</td>\n",
       "      <td>4</td>\n",
       "      <td>Mother</td>\n",
       "      <td>20.131830</td>\n",
       "      <td>16851507</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "      <td>ADV</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61453075</td>\n",
       "      <td>merrily</td>\n",
       "      <td>2</td>\n",
       "      <td>Mother</td>\n",
       "      <td>20.131830</td>\n",
       "      <td>16851507</td>\n",
       "      <td>1</td>\n",
       "      <td>merrily merrily merrily merrily</td>\n",
       "      <td>adv</td>\n",
       "      <td>ADV</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>61643679</td>\n",
       "      <td>'s</td>\n",
       "      <td>8</td>\n",
       "      <td>Mother</td>\n",
       "      <td>19.033245</td>\n",
       "      <td>16904905</td>\n",
       "      <td>0</td>\n",
       "      <td>my niece's birthday was Monday and my nephew's...</td>\n",
       "      <td>poss</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>61529288</td>\n",
       "      <td>Noah</td>\n",
       "      <td>2</td>\n",
       "      <td>Mother</td>\n",
       "      <td>24.821865</td>\n",
       "      <td>16873728</td>\n",
       "      <td>0</td>\n",
       "      <td>the Noah's ark you have yeah</td>\n",
       "      <td>adj</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NounType=Prop|Number=Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>61529288</td>\n",
       "      <td>'s</td>\n",
       "      <td>2</td>\n",
       "      <td>Mother</td>\n",
       "      <td>24.821865</td>\n",
       "      <td>16873728</td>\n",
       "      <td>0</td>\n",
       "      <td>the Noah's ark you have yeah</td>\n",
       "      <td>poss</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>61971711</td>\n",
       "      <td>picture</td>\n",
       "      <td>17</td>\n",
       "      <td>Mother</td>\n",
       "      <td>36.822111</td>\n",
       "      <td>16979363</td>\n",
       "      <td>0</td>\n",
       "      <td>okay let's put them all take them out and put ...</td>\n",
       "      <td>n</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>Number=Sing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>61971711</td>\n",
       "      <td>'s</td>\n",
       "      <td>17</td>\n",
       "      <td>Mother</td>\n",
       "      <td>36.822111</td>\n",
       "      <td>16979363</td>\n",
       "      <td>0</td>\n",
       "      <td>okay let's put them all take them out and put ...</td>\n",
       "      <td>cop</td>\n",
       "      <td>PART</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1102 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_id     gloss  token_order speaker_role  target_child_age  \\\n",
       "0     62438075      this            1       Mother         29.624838   \n",
       "1     61402650      need            3       Mother         40.329370   \n",
       "2     62051390  anything            4       Mother         41.920779   \n",
       "3     61453077   merrily            4       Mother         20.131830   \n",
       "4     61453075   merrily            2       Mother         20.131830   \n",
       "...        ...       ...          ...          ...               ...   \n",
       "1097  61643679        's            8       Mother         19.033245   \n",
       "1098  61529288      Noah            2       Mother         24.821865   \n",
       "1099  61529288        's            2       Mother         24.821865   \n",
       "1100  61971711   picture           17       Mother         36.822111   \n",
       "1101  61971711        's           17       Mother         36.822111   \n",
       "\n",
       "      utterance_id  log_frequency_bin  \\\n",
       "0         17084600                  4   \n",
       "1         16839687                  4   \n",
       "2         16986849                  2   \n",
       "3         16851507                  1   \n",
       "4         16851507                  1   \n",
       "...            ...                ...   \n",
       "1097      16904905                  0   \n",
       "1098      16873728                  0   \n",
       "1099      16873728                  0   \n",
       "1100      16979363                  0   \n",
       "1101      16979363                  0   \n",
       "\n",
       "                                        utterance_gloss part_of_speech  \\\n",
       "0                   this one looks like she's driving a            pro   \n",
       "1                                            now I need              v   \n",
       "2           did we see anything else when we were there            pro   \n",
       "3                       merrily merrily merrily merrily            adv   \n",
       "4                       merrily merrily merrily merrily            adv   \n",
       "...                                                 ...            ...   \n",
       "1097  my niece's birthday was Monday and my nephew's...           poss   \n",
       "1098                       the Noah's ark you have yeah            adj   \n",
       "1099                       the Noah's ark you have yeah           poss   \n",
       "1100  okay let's put them all take them out and put ...              n   \n",
       "1101  okay let's put them all take them out and put ...            cop   \n",
       "\n",
       "     tagger_part_of_speech          tagger_morphology  \n",
       "0                      DET   Number=Sing|PronType=Dem  \n",
       "1                     VERB    Tense=Pres|VerbForm=Fin  \n",
       "2                     PRON                Number=Sing  \n",
       "3                      ADV                             \n",
       "4                      ADV                             \n",
       "...                    ...                        ...  \n",
       "1097                  PART                             \n",
       "1098                 PROPN  NounType=Prop|Number=Sing  \n",
       "1099                  PART                             \n",
       "1100                  NOUN                Number=Sing  \n",
       "1101                  PART                             \n",
       "\n",
       "[1102 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['prefix','stem','actual_phonology','model_phonology','corpus_name','suffix','clitic','num_morphemes','english','language','speaker_id','target_child_id','utterance_type','transcript_id','speaker_code','speaker_name','target_child_name','target_child_sex','collection_name','collection_id','corpus_id','frequency','log_frequency','split_index']\n",
    "final_samples_filtered = final_samples.drop(columns=columns_to_drop)\n",
    "\n",
    "final_samples_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_samples_filtered.to_csv(TAGGED_SAMPLED_TOKENS_CSV_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "childes_py3",
   "language": "python",
   "name": "childes_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
